# -*- coding: utf-8 -*-
"""Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AN1R6h7VALNoiWjvFBYKpZWCeOd9v8e_
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version

# Commented out IPython magic to ensure Python compatibility.
#       import the libraries
import pandas as pd
import numpy as np
from glob import glob
import matplotlib.pyplot as plt
# %matplotlib inline

#import layers from keras
from keras.layers import Dense, InputLayer
from keras.models import Sequential
from tensorflow.keras.optimizers import Adam

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#   unzip contents of the drive
!unzip /content/drive/MyDrive/train_nLPp5K8.zip



#   load the data
image_data = pd.read_csv('/content/train.csv', nrows = 2000)

image_data.shape

image_data.head()

#   create random number generation, to keep the same pattern of randomization
seed = 42
range = np.random.RandomState(seed)

image_data.head()

#   Load images and store them in numpy array
x = []  # list to store the images
for img_name in image_data.image_names:
  img = plt.imread('/content/images/'+img_name)
  x.append(img)

# dependant variable
y = image_data['class'].values   #target second column of csv file

#   convert list to array
x = np.array(x)

x.shape

"""PRE-PR0CESSING THE DATA"""

#   convert the 3d image to 1d image
x = x.reshape(x.shape[0], 224*224*3)

x.shape

#   the images are now 1 dimensional
# minimum and maximum of pixel values of the images
x.min(), x.max()

x=x.astype('float32')

#   normalizing the pixel values of images [converting to the 0 and 1]
x = x/x.max()

x.min(), x.max()

"""TRAINING AND VALIDATION"""

x_train, x_valid, y_train, y_valid = train_test_split(x,y, test_size = 0.4, random_state = seed)

"""DEFINING THE MODEL ARCHITECTURE"""

model = Sequential()
model.add(InputLayer(input_shape = (224*224*3)))
model.add(Dense(100, activation = 'sigmoid'))  #hidden layer has 100 neurons
model.add(Dense(100, activation = 'sigmoid'))
model.add(Dense(units = 1, activation = 'sigmoid'))

model.summary()

"""COMPILING THE MODEL"""

# defining the optimizer and setting the learning rate at 10^-5
adam = Adam(lr = 1e-5)

#   compiling the model

# define loss as binary_crossentrophy
# define optimizer as Adam
# define metrics as accuracy

model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

"""SETTING UP EARLY STOPPING"""

#   import modules for early stoppping
#       Early Stopping monitors the performance of the model for every epoch on a held-out validation
#       set during the training, and terminate the training conditional on the validation performance
#       mode = minimum, due to loss

from keras.callbacks import EarlyStopping

#   val_loss = error
#   min_delta = minimum error interval  before process is stopped
#   pantience = by the time of error interval (0.01) occurs 5 consecutive times, the early stopping rule will invoke [terminate iteration]
early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0.01, patience = 5, mode = 'min')



"""TRAIN MODEL USING EARLY STOPPING"""

#   training model for 25 epoches

model_history = model.fit(x_train, y_train, epochs = 25, batch_size = 128, validation_data = (x_valid, y_valid))

"""EVALUATING THE PERFORMANCE"""

from sklearn.ensemble import RandomForestRegressor

# accuracy on validation set
predict_x = model.predict(x_valid)
classes_x = np.argmax(predict_x, axis = 1)

clf = RandomForestRegressor(n_estimators = 10)

#       print the graphs
#   loss graph
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc = 'upper right')
plt.show

#     accuracy graph
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model_accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc = 'lower right')
plt.show

#       PRINT PROBABILITYAND ACTUAL CLASS
# pull out the original images from the data which correspond to the validation data
_, gender , _, valid_y = train_test_split(image_data.image_names.values, y, test_size=0.3, random_state=seed)

# get a random index to plot image randomly
index = range.choice(len(gender))

# get the corresponding image name and probability
img_name = gender[index]
prob = (predict_x * 100).astype(int)[index] # used to determine probabulity of males to females/females to males
# read the image
img = plt.imread('images/' + img_name)

"""PRINT PROBABILITY"""

# print probability and actual class
print('Gender is:', prob,  '% male')
print('Gender is: ',100-prob, '% female ')
print('And actual class is ', valid_y[index])

# plot image
plt.imshow(img)

"""HYPERPARAMETER TUNING OF NEURAL NETWORK"""

#     Change activation function of hidden layer
# using relu as activation function in hidden layer
model=Sequential()
model.add(InputLayer(input_shape=(x_train.shape[1],)))
model.add(Dense(100, activation='sigmoid'))
model.add(Dense(units=1, activation='sigmoid'))

# compiling the model
model.compile(loss='binary_crossentropy', optimizer="adam", metrics=['accuracy'])

# summary of the model
model.summary()

# training the model for 25 epochs
model_history = model.fit(x_train, y_train, epochs=25, batch_size=128,validation_data=(x_valid,y_valid))

"""VISUALISING ACCURACY OF THE MODEL"""

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()

"""INCREASE HIDDEN NEURONS"""

# increase hidden neurons
model=Sequential()
model.add(InputLayer(input_shape=(x_train.shape[1],)))
model.add(Dense(1024, activation='sigmoid'))
model.add(Dense(units=1, activation='sigmoid'))

# compiling the model
model.compile(loss='binary_crossentropy', optimizer="adam", metrics=['accuracy'])

# model summary
model.summary()

model_history = model.fit(x_train, y_train, epochs=25, batch_size=128,validation_data=(x_valid,y_valid))

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""INCREASE HIDDEN LAYERS"""

# increase hidden layers
model=Sequential()
model.add(InputLayer(input_shape=(x_train.shape[1],)))
model.add(Dense(100, activation='sigmoid'))
model.add(Dense(100, activation='sigmoid'))
model.add(Dense(units=1, activation='sigmoid'))

# compiling the model
model.compile(loss='binary_crossentropy', optimizer="adam", metrics=['accuracy'])

# model summary
model.summary()

model_history = model.fit(x_train, y_train, epochs=25, batch_size=128,validation_data=(x_valid,y_valid))

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()

"""INCREASING NUMBER OF EPACHS"""

# increase number of epochs
model=Sequential()
model.add(InputLayer(input_shape=(x_train.shape[1],)))
model.add(Dense(100, activation='sigmoid'))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer="adam", metrics=['accuracy'])

# model summary
model.summary()

model_history = model.fit(x_train, y_train, epochs=50, batch_size=128,validation_data=(x_valid,y_valid))

"""THE ACCURACY IS ALWAYS CONSTANT, THERES A SLIGHT DIFFERENCE IN THE LOSS VALUES"""

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

"""CHANGING THE OPTIMIZER"""

# change the optimizer
model=Sequential()
model.add(InputLayer(input_shape=(x_train.shape[1],)))
model.add(Dense(100, activation='sigmoid'))
model.add(Dense(units=1, activation='sigmoid'))

#  optimizer as sgd
model.compile(loss='binary_crossentropy', optimizer="sgd", metrics=['accuracy'])

# model summary
model.summary()

model_history = model.fit(x_train, y_train, epochs=25, batch_size=128,validation_data=(x_valid,y_valid))

""" For the sgd optimizer, the loss values remain flactuating, however the accuracy flactuates [fluctuation values are low at lower epocher and increase as higher epoches]"""

# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()



